\chapter{Literature Review}
\label{ch:literature-review}

This chapter establishes the theoretical and empirical foundation for integrating a resource-augmented AI assistant into Sovelia Core PLM. We begin by examining the fundamental characteristics of Product Lifecycle Management systems and the role of AI assistants in data-intensive workflows, establishing why a human-in-the-loop approach is essential for enterprise PLM environments (\autoref{sec:plm-systems-and-ai-assistants}). We then explore the architecture and principles of Retrieval-Augmented Generation (RAG) systems, focusing on how the combination of parametric and non-parametric memory addresses key limitations of purely parametric models in knowledge-intensive tasks (\autoref{sec:resource-augmented-ai-systems}).

The core of this review examines four recent case studies of RAG-based systems deployed in technical documentation and manufacturing contexts (\autoref{sec:case-studies-on-similar-integrations}): Lin et al.'s semiconductor manufacturing assistant, Shejuti et al.'s technical documentation chatbot, Nam et al.'s multimodal automotive maintenance system, and Wang et al.'s comprehensive framework for AI in PLM. These implementations reveal both common success factors—domain-specific adaptation, parameter-efficient fine-tuning, multimodal integration—and persistent challenges including manual annotation overhead, retrieval precision issues, and the need for procedural accuracy metrics beyond standard NLP benchmarks. By synthesizing lessons from these deployments, we identify key design considerations and risk factors that inform the architecture and implementation strategy for Sovelia Core's AI assistant, particularly given the constraints of on-premise deployment and data governance requirements.

\section{Overview of PLM systems and AI assistants}
\label{sec:plm-systems-and-ai-assistants}

Product Lifecycle Management (PLM) systems are integral to managing the entire lifecycle of a product from inception, through engineering design and manufacturing, to service and disposal. They provide a centralized repository for all product-related information, facilitating collaboration among different departments and stakeholders \parencite{stark_product_2015}. In PLM systems generally the lifecycle states of product include: imagine, define, realise, support/use and retire/dispose \parencite{stark_product_2015-1}. All these separate stages generate data (including documentation) that needs to be managed effectively. The purpose of PLM system is centralize all this data and provide users with access to the right information at the right time.

AI assistants, in the context of data-intensive workflows, are defined as \textit{semi-automatic interactive tools} that guide analysts through specific tasks by recommending suitable transformations or actions that respect constraints obtained through interaction with the analyst \parencite{petricek_ai_2023}. Unlike fully automatic systems that attempt to solve problems without human intervention, or purely manual tools that require complete human control, AI assistants implement an iterative interaction pattern where the system makes an initial recommendation, the user can provide feedback through structured constraints or selections, and the system refines its recommendations accordingly. This human-in-the-loop approach combines the scalability and automation of machine learning with critical human insight, making it particularly suitable for tasks where edge cases and domain-specific knowledge are crucial \parencite{petricek_ai_2023}. In this thesis, the aim is to develop an AI assistant that can be integrated into PLM system to enhance user experience by providing context-aware support, streamlining routine data wrangling tasks, and improving access to information while maintaining human oversight and control.

\section{Resource-augmented AI systems}
\label{sec:resource-augmented-ai-systems}

Resource-augmented AI systems, also known as \emph{retrieval-augmented generation} (RAG) systems, represent a paradigm shift in how artificial intelligence systems access and utilize knowledge. Unlike purely parametric models that store all knowledge implicitly within their parameters, resource-augmented systems combine parametric memory (the weights of a neural network) with non-parametric memory (external knowledge sources such as databases, document collections, or knowledge graphs) \parencite{lewis_retrieval-augmented_2021}. This hybrid architecture addresses several fundamental limitations of parametric-only models: the inability to easily update or revise stored knowledge, difficulty in providing provenance for predictions, and the tendency to generate hallucinated or factually incorrect content \parencite{lewis_retrieval-augmented_2021}.

In a typical RAG architecture, the system consists of two main components: a \emph{retriever} that identifies relevant documents or passages from an external knowledge source given an input query, and a \emph{generator} that produces outputs conditioned on both the input and the retrieved content \parencite{lewis_retrieval-augmented_2021}. The retriever typically employs dense passage retrieval methods using bi-encoder architectures that compute semantic similarity between queries and documents in a shared embedding space, while the generator is commonly implemented as a pre-trained sequence-to-sequence transformer model. Crucially, these components can be trained end-to-end, allowing the retrieval mechanism to learn what information is relevant for the downstream task without requiring explicit retrieval supervision.

Lewis et al. \parencite{lewis_retrieval-augmented_2021} have highlighted that the advantages of resource-augmented systems are particularly pronounced in knowledge-intensive tasks—tasks that humans could not reasonably perform without access to external knowledge sources. By maintaining knowledge in an explicit, inspectable, and easily updatable non-parametric form, these systems enable dynamic knowledge updates by simply replacing or modifying the external knowledge source (a process sometimes called "index hot-swapping") without requiring costly model retraining. Furthermore, the retrieved documents provide a form of interpretability and provenance, allowing users to understand what information informed the system's response. In the context of PLM systems, this architecture is particularly well-suited for handling the extensive, evolving documentation ecosystem that characterizes enterprise software deployments.

\section{Case studies on similar integrations}
\label{sec:case-studies-on-similar-integrations}

Several recent implementations of RAG-based systems for technical documentation and manufacturing environments provide valuable insights for the design and development of an AI assistant for Sovelia Core PLM. This section examines four representative case studies that demonstrate both the feasibility and challenges of integrating RAG technology into complex technical domains.

\subsection{Semiconductor manufacturing: Lin et al.'s Intelligent Manufacturing Virtual Assistant}

Lin et al. (2025) developed an Intelligent Manufacturing Virtual Assistant (IMVA) specifically designed for the semiconductor industry, addressing challenges similar to those faced in PLM environments \parencite{lin_generative_2025}. The semiconductor manufacturing domain shares several characteristics with PLM systems: extensive technical documentation spanning thousands of pages, multiple specialized subsystems requiring expert knowledge (yield analysis systems, fault detection systems, equipment health monitoring), and the need for rapid access to procedural information by personnel with varying levels of expertise.

The IMVA system implemented a multimodal RAG architecture combining text and image retrieval from maintenance manuals, with documents processed into 1000-character chunks and embedded using the BAAI-bge-m3 model. The system architecture consisted of dual pipelines: a Large Language Model training pipeline using domain continual pretraining on maintenance descriptions, and a RAG training pipeline with contrastive learning for semantic document search. Notably, Lin et al. employed LoRA-based parameter-efficient fine-tuning (training only 0.1\% of total parameters) to adapt the bLLossom-8B model for maintenance-specific question answering.

The IMVA achieved practical deployment in an Infineon Technology facility, demonstrating significant operational improvements in the Wide-bandgap (WBG) semiconductor process. In a real-world validation scenario, the system successfully identified equipment performance issues (EPISIC9-PM2 showing high scheduled downtime, low MTBF, and highest defect density) and traced root causes through automated log analysis, pinpointing temperature control power-off events and hardware interlock failures. The mean time between failures (MTBF) improved from 83.22 days to approximately 100 days following IMVA-guided interventions.

For the Sovelia Core PLM integration, Lin et al.'s work demonstrates several applicable lessons: the viability of LoRA-based fine-tuning for resource-constrained environments (critical for on-premise deployments), the importance of multimodal retrieval for technical documentation, and the necessity of domain-specific continual pretraining. However, their implementation revealed challenges in handling complex multi-condition queries and emphasized the need for manual curation of image-text mappings (only 200 high-quality pairs), suggesting that fully automated multimodal alignment remains an open challenge.

\subsection{Technical documentation: Shejuti et al.'s MODTRAN chatbot}

Shejuti et al. (2025) addressed the problem of navigating extensive technical documentation through their RAG-based chatbot for MODTRAN (Moderate resolution atmospheric TRANsmission) software, a domain characterized by complex scientific documentation similar to specialized PLM system manuals \parencite{shejuti_extended_2025}. The MODTRAN documentation ecosystem included a 522-page user manual, algorithm theoretical basis document (ATBD), and FAQ resources—a documentation complexity comparable to enterprise PLM deployments with multiple document types serving different user needs.

The system architecture employed PyPDF2 for PDF text extraction while preserving hierarchical structure, BeautifulSoup for HTML parsing of FAQ pages, and CharacterTextSplitter to segment documents into 1000-character chunks with 200-character overlap. Embeddings were generated using HuggingFace's SciBERT-NLI model specifically selected for scientific document understanding, and stored in a FAISS vector database. The RAG pipeline retrieved top-k=13 chunks based on cosine similarity, passing them to an LLM through LangChain's load\_qa\_chain.

Evaluation with 20 domain experts yielded an average satisfaction score of 4.4 out of 5, with particularly strong performance in response speed (4.6) and information clarity (4.5). However, the system demonstrated lower scores for system reliability (4.2), and expert feedback revealed occasional extraneous information in responses, indicating retrieval precision challenges. Comparative testing against ChatGPT showed that the domain-adapted RAG system produced more concise, keyword-focused answers aligned with expert expectations, whereas general-purpose LLMs provided verbose but less specific responses.

For Sovelia Core PLM, this case study highlights the importance of domain-specific embedding models (SciBERT for scientific texts, potentially requiring a similar engineering/PLM-domain model), the effectiveness of relatively simple retrieval architectures when properly tuned, and the critical role of chunk size and overlap parameters in balancing context completeness with retrieval precision. The study also underscores the need for iterative refinement based on expert evaluation, as initial implementations may suffer from retrieval noise affecting response quality.

\subsection{Automotive maintenance: Nam et al.'s multimodal RAG for Hyundai Staria}

Nam et al. (2025) developed a domain-adaptive multimodal RAG system for Hyundai Staria technical manuals, representing the most directly comparable case study to PLM system integration due to its focus on maintenance documentation, procedural accuracy, and multimodal content \parencite{nam_lora-tuned_2025}. The Staria manual dataset comprised 522 pages with 1.06 MB of text and 878 images, processed through a systematic pipeline including PDF extraction using PyMuPDF, hierarchical structure identification (748 upper-level sections with sporadic lower-level subsections), and manual annotation of 200 high-quality image-text pairs.

The system architecture incorporated sentence-level similarity learning based on manual annotations from five automotive maintenance experts (inter-rater agreement ICC = 0.87), with contrastive learning applied to the BAAI-bge-m3 embedding model using MultipleNegativesRankingLoss. The generator employed LoRA fine-tuning (rank=64, alpha=128, dropout=0.1) on the bLLossom-8B model, trained on three dataset types: Simple QA (single-turn factual questions), RAG QA (5,065 context-augmented pairs), and Multi-Turn QA (275 conversational scenarios averaging 3.2 dialogue turns).

Quantitative evaluation demonstrated substantial improvements over baseline models: BERTScore increased from 75.10\% to 78.11\% (+3.01 percentage points), cosine similarity improved from 75.81\% to 78.11\% (+2.30 pp), and ROUGE-L showed dramatic gains from 9.09\% to 27.12\% (+18.03 pp), indicating significantly better lexical accuracy and technical terminology alignment. Procedural accuracy reached 89.7\%, a critical metric for maintenance applications. User satisfaction evaluation by 20 military maintenance personnel yielded an overall score of 4.4, with multimodal responses showing 0.92-point improvement over text-only responses in information clarity.

The lessons for Sovelia Core PLM are particularly salient: the necessity of multi-turn dialogue capability for realistic troubleshooting scenarios (users rarely formulate complete, single-turn queries), the quantifiable benefit of multimodal integration despite significant manual annotation overhead, and the importance of procedural accuracy metrics beyond standard NLP evaluation measures. However, Nam et al. identified scalability bottlenecks in manual image-text curation (suggesting future semi-automated alignment approaches) and noted that cross-domain generalization remains empirically unvalidated—a concern directly relevant to adapting their methodology across different PLM environments or equipment types.

\subsection{AI in Product Lifecycle Management: Wang et al.'s comprehensive review}

Wang et al. (2021) provided a holistic framework for understanding AI applications across the entire product lifecycle, offering strategic insights for integrating intelligent systems into PLM stages: design, manufacturing, and service \parencite{wang_artificial_2021}. While not a RAG-specific implementation, their systematic review of AI technologies in PLM contexts establishes the broader landscape within which RAG-based assistants operate.

In the design stage, Wang et al. identified AI applications in market analysis through data mining (discovering unmet requirements and predicting market opportunities), rapid conceptual design using case libraries with deep convolutional networks for design fragment identification, and parameter recommendation through expert systems integrated with CAD platforms. The manufacturing stage incorporates AI for intelligent supplier selection (fuzzy theory and heuristic algorithms for multi-attribute decision-making), advanced planning and scheduling (APS systems augmented with evolutionary algorithms), and quality assessment using deep learning for automated inspection.

For the service stage—most relevant to PLM documentation support—Wang et al. highlighted intelligent customer service combining semantic retrieval (natural language processing for knowledge base construction) and expert systems for knowledge matching, product status monitoring through distributed computing with intelligent algorithms, and failure prediction using machine learning models (decision trees, neural networks, Kalman filters) for predictive health management.

The framework reveals critical integration points for RAG systems in PLM: the need to bridge design databases, case libraries, and knowledge libraries (similar to the knowledge sources a RAG retriever must access); the importance of domain-specific fine-tuning and knowledge representation (expert systems as a precursor to modern RAG architectures); and the challenge of data integration across heterogeneous PLM stages. Wang et al. emphasized that data quality, algorithm interpretability, and security (information, equipment, and trust authentication) remain primary challenges—concerns that directly impact RAG deployment in enterprise PLM environments.

For Sovelia Core, this comprehensive view suggests that a RAG assistant must not only provide document retrieval but also integrate with existing PLM information systems (design databases, manufacturing execution systems, customer relationship management), support diverse query types spanning multiple lifecycle stages, and maintain transparency in its retrieval and generation process to build user trust. The review also underscores the importance of considering deployment constraints (on-premise vs. cloud, data sovereignty requirements) and the need for human-in-the-loop mechanisms for validation and feedback—principles central to responsible AI assistant design.

\subsection{Synthesis and implications for Sovelia Core PLM}

Collectively, these case studies demonstrate both the technical feasibility and practical challenges of integrating RAG-based AI assistants into complex technical documentation environments. Common success factors include: (1) domain-specific embedding models or fine-tuning strategies (SciBERT for scientific documentation, maintenance-specific training for industrial applications); (2) parameter-efficient adaptation methods (LoRA fine-tuning) enabling deployment in resource-constrained settings; (3) careful attention to chunk size, retrieval parameters, and multimodal alignment; and (4) iterative refinement based on expert user feedback.

However, persistent challenges emerge across implementations: manual annotation overhead for multimodal content limiting scalability, difficulty handling complex multi-condition queries requiring deep procedural reasoning, retrieval precision issues causing extraneous information in responses, and the need for procedural accuracy metrics beyond standard NLP benchmarks. For Sovelia Core PLM, these insights suggest prioritizing semi-automated approaches for image-text alignment, implementing multi-turn dialogue capability for realistic user interactions, developing domain-specific evaluation metrics for procedural correctness, and designing human-in-the-loop feedback mechanisms to continuously improve system performance. Furthermore, the on-premise deployment constraint in Sovelia Core environments necessitates particular attention to resource-efficient model architectures and local data governance, lessons reinforced by the LoRA-based approaches in Lin et al. and Nam et al.'s industrial deployments.

