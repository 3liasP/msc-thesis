\chapter{Literature review}
\label{ch:literature-review}

This chapter establishes the theoretical and empirical foundation and background for integrating a resource-augmented AI assistant into Sovelia Core PLM. The review begins by examining the fundamental characteristics of Product Lifecycle Management systems and the role of AI assistants in data-intensive workflows, establishing why a human-in-the-loop approach is essential for enterprise PLM environments (\autoref{sec:plm-systems-and-ai-assistants}). After this,the architecture and principles of Retrieval-Augmented Generation (RAG) systems is synthesized, focusing on how the combination of parametric and non-parametric memory addresses key limitations of purely parametric models in knowledge-intensive tasks (\autoref{sec:resource-augmented-ai-systems}).

The core of this review examines four recent case studies of RAG-based systems deployed in technical documentation and manufacturing contexts (\autoref{sec:case-studies-on-similar-integrations}): Lin et al.'s semiconductor manufacturing assistant, Shejuti et al.'s technical documentation chatbot, Nam et al.'s multimodal automotive maintenance system, and Wang et al.'s comprehensive framework for AI in PLM. These implementations reveal both common success factors, for example domain-specific adaptation, parameter-efficient fine-tuning, multimodal integration, and persistent challenges including manual annotation overhead, retrieval precision issues, and the need for procedural accuracy metrics beyond standard NLP benchmarks. These contexts are comparable to typical PLM system users, which are usually in the manufacturing industry.\parencite{stark_product_2015} Lessons from these deployments are combined and examined to identify the most important design considerations and risk factors that could help with the architecture and implementation strategy of Sovelia Core's AI assistant, especially given the typical challenges that companies manufacturing industry face.

\section{Overview of PLM systems and AI assistants}
\label{sec:plm-systems-and-ai-assistants}

Product Lifecycle Management (PLM) systems are integral to managing the entire lifecycle of a product from inception, through engineering design and manufacturing, to service and disposal. They provide a centralized repository for all product-related information, facilitating collaboration among different departments and stakeholders \parencite{stark_product_2015}. In PLM systems generally the lifecycle states of product include: imagine, define, realise, support/use and retire/dispose \parencite{stark_product_2015-1}. All these separate stages generate data (including documentation) that needs to be managed effectively. The purpose of PLM system is to centralize all this data and provide users with access to the right information at the right time.

AI assistants, in the context of data-intensive workflows, can be defined as \textit{semi-automatic interactive tools} that guide analysts through specific tasks by recommending suitable transformations or actions that respect constraints obtained through interaction with the analyst \parencite{petricek_ai_2023}. Analysts in this context mean the human utilizing the AI assistant. Unlike fully automatic systems that attempt to solve problems without human intervention, or purely manual tools that require complete human control, AI assistants implement an iterative interaction pattern where the system makes an initial recommendation, the user can provide feedback through structured constraints or selections, and the system refines its recommendations accordingly. This human-in-the-loop approach combines the scalability and automation of machine learning with critical human insight, making it particularly suitable for tasks where edge cases and domain-specific knowledge are crucial \parencite{petricek_ai_2023}. In this thesis, the aim is to develop an AI assistant that can be integrated into PLM system to enhance user experience by providing context-aware support, streamlining routine data wrangling tasks, and improving access to information while maintaining human oversight and control. The AI assistant provide users with answers based on the documentation stored within the PLM system's database. However, the AI assistant in the context of this thesis is not fully automatic, as the users still have control over the final decisions and actions taken based on the AI assistant's recommendations.

\section{Resource-augmented AI systems}
\label{sec:resource-augmented-ai-systems}

Resource-augmented AI systems, also known as \emph{retrieval-augmented generation} (RAG) systems, represent a paradigm shift in how artificial intelligence systems access and utilize knowledge. Unlike purely parametric models that store all knowledge implicitly within their parameters, resource-augmented systems combine parametric memory (the weights of a neural network) with non-parametric memory (external knowledge sources such as databases, document collections, or knowledge graphs) \parencite{lewis_retrieval-augmented_2021}. This hybrid architecture addresses several fundamental limitations of parametric-only models: the inability to easily update or revise stored knowledge, difficulty in providing provenance for predictions, and the tendency to generate hallucinated or factually incorrect content \parencite{lewis_retrieval-augmented_2021}.

In a typical RAG architecture, the system consists of two main components: a \emph{retriever} that identifies relevant documents or passages from an external knowledge source given an input query, and a \emph{generator} that produces outputs conditioned on both the input and the retrieved content \parencite{lewis_retrieval-augmented_2021}. The retriever typically employs dense passage retrieval methods using bi-encoder architectures that compute semantic similarity between queries and documents in a shared embedding space, while the generator is commonly implemented as a pre-trained sequence-to-sequence transformer model. Crucially, these components can be trained end-to-end, allowing the retrieval mechanism to learn what information is relevant for the downstream task without requiring explicit retrieval supervision.

\textcite{lewis_retrieval-augmented_2021} have highlighted that the advantages of resource-augmented systems are particularly pronounced in knowledge-intensive tasks. There are tasks that humans could not reasonably perform without access to external knowledge sources. By maintaining knowledge in an explicit, inspectable, and easily updatable non-parametric form, these systems enable dynamic knowledge updates by simply replacing or modifying the external knowledge source (a process sometimes called "index hot-swapping") without requiring costly model retraining. Furthermore, the retrieved documents provide a form of interpretability and provenance, allowing users to understand what information informed the system's response. In the context of PLM systems, this architecture is particularly well-suited for handling the extensive, evolving documentation ecosystem that characterizes enterprise software deployments.

\section{Case studies on similar integrations}
\label{sec:case-studies-on-similar-integrations}

Several recent implementations of RAG-based systems for technical documentation and manufacturing environments provide valuable insights for the design and development of an AI assistant for Sovelia Core PLM. This section examines four representative case studies that demonstrate both the feasibility and challenges of integrating RAG technology into complex technical domains.

\subsection{Semiconductor manufacturing: Lin et al.'s Intelligent Manufacturing Virtual Assistant}

Lin et al. (2025) developed an Intelligent Manufacturing Virtual Assistant (IMVA) specifically designed for the semiconductor industry, addressing challenges similar to those faced in PLM environments \parencite{lin_generative_2025}. The semiconductor manufacturing domain shares several characteristics with PLM systems: extensive technical documentation spanning thousands of pages, multiple specialized subsystems requiring expert knowledge (yield analysis systems, fault detection systems, equipment health monitoring), and the need for rapid access to procedural information by personnel with varying levels of expertise.

The IMVA system implemented a multimodal RAG architecture combining text and image retrieval from maintenance manuals, with documents processed into 1000-character chunks and embedded using the BAAI-bge-m3 model. The system architecture consisted of dual pipelines: a Large Language Model training pipeline using domain continual pretraining on maintenance descriptions, and a RAG training pipeline with contrastive learning for semantic document search. Notably, Lin et al. employed LoRA-based parameter-efficient fine-tuning (training only 0.1\% of total parameters) to adapt the bLLossom-8B model for maintenance-specific question answering.

The IMVA achieved practical deployment in an Infineon Technology facility, demonstrating significant operational improvements in the Wide-bandgap (WBG) semiconductor process. In a real-world validation scenario, the system successfully identified equipment performance issues (EPISIC9-PM2 showing high scheduled downtime, low MTBF, and highest defect density) and traced root causes through automated log analysis, pinpointing temperature control power-off events and hardware interlock failures. The mean time between failures (MTBF) improved from 83.22 days to approximately 100 days following IMVA-guided interventions.

For the Sovelia Core PLM integration, Lin et al.'s work provides valuable insights into advanced RAG deployment in industrial settings. However, their implementation represents a more sophisticated approach than what is scoped for this thesis. The IMVA system's multimodal capabilities (combining text and image retrieval) and parameter-efficient fine-tuning methods, while demonstrating significant operational improvements, introduce substantial complexity through manual curation of image-text mappings (only 200 high-quality pairs) and domain-specific continual pretraining. These advanced techniques, though promising for future iterations, fall outside the scope of the initial text-based RAG system developed in this thesis. The core contribution of Lin et al.'s work to this research lies in their fundamental RAG architecture and retrieval strategies, which remain directly applicable to a simpler, text-focused implementation. Multimodal capabilities and parameter-efficient fine-tuning represent future enhancement opportunities once the baseline text-based system is validated in production use.

\subsection{Technical documentation: Shejuti et al.'s MODTRAN chatbot}

Shejuti et al. (2025) addressed the problem of navigating extensive technical documentation through their RAG-based chatbot for MODTRAN (Moderate resolution atmospheric TRANsmission) software, a domain characterized by complex scientific documentation similar to specialized PLM system manuals \parencite{shejuti_extended_2025}. The MODTRAN documentation ecosystem included a 522-page user manual, algorithm theoretical basis document (ATBD), and FAQ resources. This documentation complexity is somewhat comparable to enterprise PLM deployments with multiple document types serving different user needs.

The system architecture employed PyPDF2 for PDF text extraction while preserving hierarchical structure, BeautifulSoup for HTML parsing of FAQ pages, and CharacterTextSplitter to segment documents into 1000-character chunks with 200-character overlap. Embeddings were generated using HuggingFace's SciBERT-NLI model specifically selected for scientific document understanding, and stored in a FAISS vector database. The RAG pipeline retrieved top-k=13 chunks based on cosine similarity, passing them to an LLM through LangChain's load\_qa\_chain.

The system was evaluated by generating a set of query-and-answer pairs by a domain expert and then comparing the LLM responses against the expert's. Conclusion was that the LLM-generated answers were mostly accurate and relevant compared to the one's provided by the expert. Additionally, comparative testing against ChatGPT showed that the domain-adapted RAG system produced more concise, keyword-focused answers aligned with expert expectations, whereas general-purpose LLMs provided verbose but less specific responses.

For Sovelia Core PLM, this case study highlights the effectiveness of relatively simple retrieval architectures with modern pre-trained embedding models, and the importance of chunk size and overlap parameters in balancing context completeness with retrieval precision. The study also emphasizes the need for iterative refinement based on expert evaluation, as initial implementations may suffer from retrieval noise affecting response quality. All in all, this case study is highly relevant to Sovelia Core PLM due to the similarity in documentation complexity and user needs.

\subsection{Automotive maintenance: Nam et al.'s multimodal RAG for Hyundai Staria}

Nam et al. (2025) developed a domain-adaptive multimodal RAG system for Hyundai Staria technical manuals, representing the most directly comparable case study to PLM system integration due to its focus on maintenance documentation, procedural accuracy, and multimodal content \parencite{nam_lora-tuned_2025}. The Staria manual dataset comprised 522 pages with 1.06 MB of text and 878 images, processed through a systematic pipeline including PDF extraction using PyMuPDF, hierarchical structure identification (748 upper-level sections with sporadic lower-level subsections), and manual annotation of 200 high-quality image-text pairs.

The system architecture incorporated sentence-level similarity learning based on manual annotations from five automotive maintenance experts (inter-rater agreement ICC = 0.87), with contrastive learning applied to the BAAI-bge-m3 embedding model using MultipleNegativesRankingLoss. The generator employed LoRA fine-tuning (rank=64, alpha=128, dropout=0.1) on the bLLossom-8B model, trained on three dataset types: Simple QA (single-turn factual questions), RAG QA (5,065 context-augmented pairs), and Multi-Turn QA (275 conversational scenarios averaging 3.2 dialogue turns).

Quantitative evaluation demonstrated substantial improvements over baseline models: BERTScore increased from 75.10\% to 78.11\% (+3.01 percentage points), cosine similarity improved from 75.81\% to 78.11\% (+2.30 pp), and ROUGE-L showed dramatic gains from 9.09\% to 27.12\% (+18.03 pp), indicating significantly better lexical accuracy and technical terminology alignment. Procedural accuracy reached 89.7\%, a critical metric for maintenance applications. User satisfaction evaluation by 20 military maintenance personnel yielded an overall score of 4.4, with multimodal responses showing 0.92-point improvement over text-only responses in information clarity.

The lessons for Sovelia Core PLM center on architectural considerations applicable to text-based RAG systems: the necessity of multi-turn dialogue capability for realistic troubleshooting scenarios (users rarely formulate complete, single-turn queries) and the importance of procedural accuracy metrics beyond standard NLP evaluation measures. While Nam et al.'s quantitative evaluation demonstrated substantial benefits from multimodal integration (0.92-point improvement in information clarity with image support), the significant manual annotation overhead (200 high-quality image-text pairs curated by five experts) and specialized fine-tuning requirements position multimodal capabilities as a future enhancement rather than an initial implementation target. For the scope of this thesis, the focus remains on establishing a robust text-based RAG foundation, with Nam et al.'s multimodal work serving as a reference architecture for subsequent iterations. The scalability bottlenecks they identified in manual image-text curation and the empirically unvalidated cross-domain generalization further reinforce the rationale for prioritizing a simpler, more maintainable text-only approach in the initial deployment.

\subsection{AI in PLM: Wang et al.'s comprehensive review}

Wang et al. (2021) provided a holistic framework for understanding AI applications across the entire product lifecycle, offering strategic insights for integrating intelligent systems into PLM stages: design, manufacturing, and service \parencite{wang_artificial_2021}. While not a RAG-specific implementation, their systematic review of AI technologies in PLM contexts establishes the broader landscape within which RAG-based assistants operate.

In the design stage, Wang et al. identified AI applications in market analysis through data mining (discovering unmet requirements and predicting market opportunities), rapid conceptual design using case libraries with deep convolutional networks for design fragment identification, and parameter recommendation through expert systems integrated with CAD platforms. The manufacturing stage incorporates AI for intelligent supplier selection (fuzzy theory and heuristic algorithms for multi-attribute decision-making), advanced planning and scheduling (APS systems augmented with evolutionary algorithms), and quality assessment using deep learning for automated inspection.

For the service stage, which is the most relevant to PLM documentation support, Wang et al. highlighted intelligent customer service combining semantic retrieval (natural language processing for knowledge base construction) and expert systems for knowledge matching, product status monitoring through distributed computing with intelligent algorithms, and failure prediction using machine learning models (decision trees, neural networks, Kalman filters) for predictive health management.

The framework reveals critical integration points for RAG systems in PLM: the need to bridge design databases, case libraries, and knowledge libraries (similar to the knowledge sources a RAG retriever must access); the importance of domain-specific fine-tuning and knowledge representation (expert systems as a precursor to modern RAG architectures); and the challenge of data integration across heterogeneous PLM stages. Wang et al. emphasized that data quality, algorithm interpretability, and security (information, equipment, and trust authentication) remain primary challenges. These are concerns that directly impact RAG deployment in enterprise PLM environments.

For Sovelia Core, this comprehensive view suggests that a RAG assistant must not only provide document retrieval but also integrate with existing PLM information systems (design databases, manufacturing execution systems, customer relationship management), support diverse query types spanning multiple lifecycle stages, and maintain transparency in its retrieval and generation process to build user trust. The review also underscores the importance of considering deployment constraints (on-premise vs. cloud, data sovereignty requirements) and the need for human-in-the-loop mechanisms for validation and feedback. These principles are central to responsible AI assistant design.

\section{Synthesis and implications for Sovelia Core PLM}

Collectively, the case studies detailed in section \ref{sec:case-studies-on-similar-integrations} demonstrate both the technical feasibility and practical challenges of integrating RAG-based AI assistants into complex technical documentation environments. For the scope of this thesis, which focuses on developing a foundational text-based RAG system, the case studies reveal several directly applicable success factors:

\begin{enumerate}
    \item The effectiveness of modern pre-trained embedding models without requiring extensive fine-tuning
    \item The importance of fundamental retrieval parameters (chunk size, overlap, top-k selection) in balancing context completeness with precision
    \item Multi-turn dialogue capability for realistic user interactions
    \item Iterative refinement based on expert user feedback
\end{enumerate}

The case studies also highlight more advanced techniques: multimodal integration (Lin et al., Nam et al.), parameter-efficient fine-tuning with LoRA (Lin et al., Nam et al.), and domain-specific continual pretraining. These approaches demonstrate significant performance improvements but introduce substantial implementation complexity. While these approaches show promise, their requirements for manual annotation (200 image-text pairs), specialized training infrastructure, and domain-specific model adaptation position them as future enhancement opportunities rather than initial implementation priorities. The persistent challenges identified across implementations, for example manual annotation overhead for multimodal content, difficulty handling complex multi-condition queries, and retrieval precision issues, further reinforce the rationale for establishing a simpler, more maintainable text-based RAG foundation before considering advanced extensions.

For Sovelia Core PLM's initial deployment, these insights suggest prioritizing:

\begin{enumerate}
    \item A straightforward retrieval architecture using pre-trained embedding models
    \item Careful tuning of fundamental parameters (chunking strategy, retrieval depth)
    \item Multi-turn conversational capability
    \item Human-in-the-loop feedback mechanisms for continuous improvement
\end{enumerate}

The on-premise deployment constraint necessitates particular attention to resource-efficient architectures and local data governance. Multimodal capabilities, parameter-efficient fine-tuning, and advanced retrieval strategies remain valuable directions for future iterations once the baseline text-based system demonstrates practical value and achieves production stability.
