\chapter{Literature review}
\label{ch:literature-review}

This chapter establishes the theoretical and empirical foundation and background for integrating a resource-augmented AI assistant into Sovelia Core PLM. The review begins by examining the fundamental characteristics of Product Lifecycle Management systems and the role of AI assistants in data-intensive workflows, establishing why a human-in-the-loop considerable approach for enterprise PLM environments (\autoref{sec:plm-systems-and-ai-assistants}). After this, the overall architecture and principles of Retrieval-Augmented Generation (RAG) systems is explained briefly, focusing on how the combination of parametric and non-parametric memory addresses key limitations of purely parametric models in knowledge-intensive tasks (\autoref{sec:resource-augmented-ai-systems}).

The core of this review examines five recent case studies of RAG-based systems deployed in technical documentation and manufacturing contexts (\autoref{sec:case-studies-on-similar-integrations}): Heredia Álvaro and Barreda's ceramic tile manufacturing quality control system, Shejuti et al.'s technical documentation chatbot, Knollmeyer et al.'s manufacturing documentation system with knowledge graph enhancement, Wan et al.'s hybrid KG-vector RAG for smart manufacturing, and Wang et al.'s comprehensive framework for AI in PLM. These studies reveal both common success factors, particularly in domain-specific adaptation, and persistent challenges. The contexts of these studies are comparable to those of typical PLM system users, who are predominantly in the manufacturing industry \parencite{stark_product_2015}. By synthesizing lessons from these deployments, this review identifies key design considerations and risk factors to inform the architecture and implementation strategy of the AI assistant for Sovelia Core, especially given the challenges faced by companies in the manufacturing industry.

\section{Overview of PLM systems and AI assistants}
\label{sec:plm-systems-and-ai-assistants}

Product Lifecycle Management (PLM) systems are integral to managing the entire lifecycle of a product from inception, through engineering design and manufacturing, to service and disposal. They provide a centralized repository for all product-related information, facilitating collaboration among different departments and stakeholders \parencite{stark_product_2015}. In PLM systems generally the lifecycle states of product include: imagine, define, realise, support/use and retire/dispose \parencite{stark_product_2015-1}. All these separate stages generate data (including documentation) that needs to be managed effectively. The purpose of PLM system is to centralize all this data and provide users with access to the right information at the right time.

AI assistants, in the context of data-intensive workflows, can be defined as \textit{semi-automatic interactive tools} that guide analysts through specific tasks by recommending suitable transformations or actions that respect constraints obtained through interaction with the analyst \parencite{petricek_ai_2023}. Analysts in this context mean the human utilizing the AI assistant. Unlike fully automatic systems that attempt to solve problems without human intervention, or purely manual tools that require complete human control, AI assistants implement an iterative interaction pattern where the system makes an initial recommendation, the user can provide feedback through structured constraints or selections, and the system refines its recommendations accordingly. This human-in-the-loop approach combines the scalability and automation of machine learning with critical human insight, making it particularly suitable for tasks where edge cases and domain-specific knowledge are crucial \parencite{petricek_ai_2023}. In this thesis, the aim is to develop an AI assistant that can be integrated into PLM system to enhance user experience by providing context-aware support, streamlining routine data wrangling tasks, and improving access to information while maintaining human oversight and control. The AI assistant is meant to provide users with answers based on the documentation stored within the PLM system's database. However, the AI assistant in the context of this thesis is not fully automatic, as the users still have control over the final decisions and actions taken based on the AI assistant's recommendations. The purpose of the AI assistant is to inform and help with decision-making, rather than replacing human judgment entirely.

\section{Resource-augmented AI systems}
\label{sec:resource-augmented-ai-systems}

Resource-augmented AI systems, also known as \emph{retrieval-augmented generation} (RAG) systems, represent a paradigm shift in how artificial intelligence systems access and utilize knowledge. Unlike purely parametric models that store all knowledge implicitly within their parameters, resource-augmented systems combine parametric memory (the weights of a neural network) with non-parametric memory (external knowledge sources such as databases, document collections, or knowledge graphs) \parencite{lewis_retrieval-augmented_2021}. This hybrid architecture addresses several fundamental limitations of parametric-only models: the inability to easily update or revise stored knowledge, difficulty in providing provenance for predictions, and the tendency to generate hallucinated or factually incorrect content \parencite{lewis_retrieval-augmented_2021}. In simplified terms, RAG systems can be viewed as AI systems that "look up" relevant information from an external source before generating a response, rather than relying solely on learned patterns from training data and thus enabling more accurate, up-to-date, and contextually relevant outputs. This will result in less hallucinations compared to standard large language models (LLMs) \parencite{lewis_retrieval-augmented_2021}.

\begin{figure}[htbp]
    \centering
    \small{\resizebox{\textwidth}{!}{\input{diagrams/rag.tex}}}
    \caption{Retrieval-Augmented Generation (RAG) system architecture}
    \label{fig:rag-architecture}
\end{figure}

In a typical RAG architecture, the system consists of two main components: a \emph{retriever} that identifies relevant documents or passages from an external knowledge source given an input query, and a \emph{generator} that produces outputs conditioned on both the input and the retrieved content \parencite{lewis_retrieval-augmented_2021}. The retriever typically employs dense passage retrieval methods using bi-encoder architectures that compute semantic similarity between queries and documents in a shared embedding space, while the generator is commonly implemented as a pre-trained sequence-to-sequence transformer model. Crucially, these components can be trained end-to-end, allowing the retrieval mechanism to learn what information is relevant for the downstream task without requiring explicit retrieval supervision. \autoref{fig:rag-architecture} illustrates this architecture, showing how user queries flow through the retriever to access the knowledge base, and how retrieved context is combined with the query in the generator to produce responses.

\textcite{lewis_retrieval-augmented_2021} have highlighted that the advantages of resource-augmented systems are particularly pronounced in knowledge-intensive tasks. There are tasks that humans could not reasonably perform without access to external knowledge sources. By maintaining knowledge in an explicit, inspectable, and easily updatable non-parametric form, these systems enable dynamic knowledge updates by simply replacing or modifying the external knowledge source (a process sometimes called "index hot-swapping") without requiring costly model retraining and allowing for customizability for different needs. Additionally, the retrieved documents allow users to understand what information informed the system's response. In the context of PLM systems, this architecture is particularly well-suited for handling the extensive, evolving documentation ecosystem that characterizes enterprise software deployments. The customizable nature of RAG systems allow for tailoring the knowledge and thus the responses to the specific configurations and processes of different organizations using the PLM system.

\section{Case studies on similar integrations}
\label{sec:case-studies-on-similar-integrations}

Several recent implementations of RAG-based systems for technical documentation and manufacturing environments provide valuable insights for the design and development of an AI assistant for Sovelia Core, the main topic of this thesis. This section examines four representative case studies that demonstrate both the feasibility and challenges of integrating RAG technology into similarly complex technical domains.

\subsection*{Manufacturing quality control: Heredia Álvaro and Barreda's ceramic tile RAG system}

Heredia Álvaro and Barreda (2025) developed an advanced RAG system for manufacturing quality control in the ceramic tile industry \parencite{heredia_alvaro_advanced_2025}, addressing challenges comparable to those in PLM environments. Notably, as any companies in the manufacturing industry, ceramic tile manufacturers can be seen as potential PLM system users and thus, face similar issues with knowledge silos in technical documentation. In the study there are many similar characteristics mentioned: extensive technical documentation (defect handbooks, process articles), multiple specialized stages requiring expert knowledge (pressing, drying, enameling, firing, finishing), and the need for rapid procedural access for defect diagnosis and root cause analysis by users with varying expertise levels.

The system architecture detailed in the study employed a straightforward RAG implementation using OpenAI's text-embedding-3-large model for document indexing, with a pre-processing pipeline that structured 221 samples of ceramic defect information (defect type, identification methods, causes, solutions, origin areas). Documents were processed through validation mechanisms, with short documents fed directly to the LLM and longer documents first fragmented and stored in a Chroma vector store. Notably, Heredia Álvaro and Barreda implemented a two-stage retrieval approach: initial retrieval using a bi-encoder with Euclidean distance similarity, followed by post-retrieval reranking using a cross-encoder (sentence transformers ms-frame-MiniLM-L-6-v2) to prioritize the most relevant information samples. This combination addresses the computational trade-offs between efficiency (bi-encoder) and accuracy (cross-encoder). The generator itself used OpenAI's gpt-3.5-turbo-instruct model with customized prompts to make sure that responses prioritized external knowledge over the model's parametric memory. \parencite{heredia_alvaro_advanced_2025}

The system demonstrated practical effectiveness after the evaluation using both retrieval and generation metrics. The retrieval phase achieved 92.68\% Jaccard similarity and 85.81\% F1-score with optimal hyperparameters (k=7 retrieved samples, mean relevance score threshold), indicating high precision in identifying relevant context. The generation phase evaluation using ROUGE-L metrics yielded a mean score of 0.6108 (standard deviation 0.1371), significantly outperforming random baselines (mean 0.2300). Qualitative comparison against GPT-4 without domain adaptation demonstrated the RAG system's superiority in providing accurate, domain-specific answers. For example, when queried about carbon particles in ceramic tiles, the RAG system correctly identified the defect as impurities in raw materials and recommended weathering clays or finer sieving, whereas GPT-4 incorrectly attributed the issue to general fouling and cleaning problems. The system operates at low computational cost (approximately \$0.0012 per query) and achieved practical deployment for use cases including customer claim resolution, non-conformities reporting, and continuous improvement actions. \parencite{heredia_alvaro_advanced_2025}

For the Sovelia Core PLM integration, Heredia Álvaro and Barreda's work provides directly applicable insights aligned with this thesis's scope. Their implementation represents a pragmatic approach prioritizing foundational RAG capabilities over advanced techniques. The architecture demonstrates several design principles relevant to enterprise PLM deployment: (1) careful pre-processing and post-processing to optimize retrieval quality without requiring model fine-tuning, (2) systematic hyperparameter optimization through evaluation metrics, (3) structured knowledge representation (defect types, causes, solutions) that is similar to PLM documentation patterns, and (4) explicit validation mechanisms to ensure query relevance. The study's emphasis on using pre-trained embedding models without extensive fine-tuning, combined with the two-stage retrieval strategy, could be used when making architectural decisions for Sovelia Core. The authors' explicit evaluation methodology of measuring both retrieval precision and generation quality provides a replicable framework for validating the PLM assistant's performance. This case study confirms the feasibility of achieving practical value with relatively simple, maintainable RAG architectures before considering more complex extensions.

\subsection*{Technical documentation: Shejuti et al.'s MODTRAN chatbot}

Shejuti et al. (2025) addressed the problem of navigating extensive technical documentation through their RAG-based chatbot for MODTRAN (Moderate resolution atmospheric TRANsmission) software, a domain characterized by complex scientific documentation similar to specialized PLM system manuals \parencite{shejuti_extended_2025}. The document selection included a MODTRAN6 user manual, algorithm theoretical basis document (ATBD), and MODTRAN FAQ resources. This documentation complexity is somewhat comparable to enterprise PLM deployments with multiple document types serving different user needs.

The system architecture employed PyPDF2 for PDF text extraction while preserving hierarchical structure, BeautifulSoup for HTML parsing of FAQ pages, and CharacterTextSplitter to segment documents into 1000-character chunks with 200-character overlap. Embeddings were generated using HuggingFace's SciBERT-NLI model specifically selected for scientific document understanding, and stored in a FAISS vector database. The RAG pipeline retrieved top-k=13 chunks based on cosine similarity, passing them to an LLM through LangChain's load\_qa\_chain.

The system was evaluated by generating a set of query-and-answer pairs by a domain expert and then comparing the LLM responses against the expert's. Conclusion was that the LLM-generated answers were mostly accurate and relevant compared to the one's provided by the expert. Additionally, comparative testing against ChatGPT showed that the domain-adapted RAG system produced more concise and focused answers that aligned with expert expectations, whereas general-purpose LLMs provided more verbose but less specific responses.

For Sovelia Core PLM, this case study again highlights the effectiveness of relatively simple retrieval architectures with modern pre-trained embedding models, and the importance of chunk size and overlap parameters in balancing context completeness with retrieval precision. The study also emphasizes the need for iterative refinement based on expert evaluation, as initial implementations may suffer from retrieval noise that affects response quality. All in all, this case study is highly relevant to Sovelia Core PLM due to the similarity in documentation complexity and user needs.

\subsection*{Manufacturing documentation: Knollmeyer et al.'s Document GraphRAG}

Knollmeyer et al. (2025) introduced Document GraphRAG, a novel framework that enhances RAG systems by incorporating knowledge graphs built upon a document's intrinsic structure into the retrieval pipeline, specifically targeting manufacturing domain documentation \parencite{knollmeyer_document_2025}. This study is particularly relevant to PLM integration as it addresses persistent challenges in retrieval precision and context selection that hinder RAG effectiveness in technical documentation environments. Notably, the research employs Design Science Research methodology, which is the same methodological approach used in this thesis, to design, implement, and evaluate the GraphRAG framework.

The system architecture leverages graph-based document structuring with a keyword-based semantic linking mechanism to improve retrieval quality beyond naive RAG baselines. Unlike the aforementioned traditional RAG systems that treat documents as flat collections of chunks, Document GraphRAG constructs knowledge graphs that preserve hierarchical document structure, section relationships, and semantic connections between content elements. The framework maintains the simplicity of text-based retrieval while adding structural awareness through graph traversal, without the need for model fine-tuning. The implementation focuses on task-dependent optimizations for fundamental parameters: chunk size, keyword density, and top-k retrieval depth. \parencite{knollmeyer_document_2025}

Evaluation on established datasets (SQuAD, HotpotQA) and a new manufacturing dataset showed consistent improvements over naive RAG baselines in retrieval and generation metrics. GraphRAG particularly enhanced context relevance for queries requiring multi-section reasoning, making it suitable for manufacturing and PLM tasks that synthesize information from related sections (e.g., process specifications and technical requirements). The manufacturing dataset confirmed its effectiveness in domain-specific question answering, with better retrieval robustness. \parencite{knollmeyer_document_2025}

For Sovelia Core PLM integration, Knollmeyer et al.'s work provides connection between theoretical RAG concepts and practical manufacturing application. The study's focus on improving retrieval precision through structural awareness rather than complex model adaptations aligns well with this thesis's pragmatic approach. Key contributions relevant to Sovelia Core include: (1) the demonstration that document structure can enhance retrieval without requiring fine-tuning, (2) systematic evaluation on manufacturing-specific datasets that validate domain transferability, (3) evidence that reasoning capabilities matter for technical documentation (PLM users might need answers that span multiple documentation sections), and (4) task-dependent parameter optimization strategies applicable to PLM deployment. The framework's emphasis on knowledge graph construction from document structure suggests a natural evolution path for PLM systems, which already maintain structured information (product hierarchies, bill-of-materials relationships, configuration dependencies). While full graph-based retrieval represents an advanced enhancement, the study confirms that foundational RAG approaches remain effective for manufacturing documentation, with graph structures offering incremental improvements for complex queries rather than fundamental architectural requirements.

\subsection*{Smart manufacturing Q\&A: Wan et al.'s hybrid KG-Vector RAG system}

Wan et al. (2025) introduced a hybrid knowledge graph (KG)-vector RAG framework specifically designed for domain-centric question answering in smart manufacturing, addressing the precision-scalability trade-off inherent in conventional RAG approaches \parencite{wan_empowering_2025}. This research is particularly relevant to PLM integration as it discusses the domain gap and outdated knowledge challenges that LLMs face in specialized manufacturing contexts. The study recognizes that conventional vector-based RAG delivers rapid responses but might lead to contextually vague results, while knowledge graph methods can offer structured and relational reasoning at the expense of scalability and efficiency. This particularly interesting and applicable to enterprise PLM deployments where both precision and performance are of high-importance.

The system architecture implements a three-stage hybrid approach that systematically integrates structured and unstructured knowledge representations. First, a metadata-including knowledge graph was constructed from documentation through systematic extraction and indexing of structured information to capture domain-specific relationships (entities, concepts, and their connections). Second, semantic alignment was achieved by injecting domain-specific constraints to enhance the contextual relevance of knowledge representations, making sure that retrieved information is aligned with manufacturing terminology. Third, a layered hybrid retrieval strategy combined the knowledge graph's explicit reasoning with the vector-based method's search power. The results were integrated through prompt engineering to produce comprehensive and context-aware responses. \parencite{wan_empowering_2025}

Evaluated on design for additive manufacturing (DfAM) tasks, the hybrid approach achieved 77.8\% exact match accuracy and 76.5\% context precision, which demonstrates improvements over baseline of vector-only and KG-only approaches. The experimental results indicated that integrating structured knowledge graph information with vector-based retrieval and prompt engineering can enhance retrieval accuracy, contextual relevance, and efficiency in LLM-based question-answering systems in smart manufacturing area. \parencite{wan_empowering_2025}

For Sovelia Core PLM integration, Wan et al.'s work provides insights into more advanced hybrid architectures that further balance precision and scalability. However, the implementation complexity makes this approach a future enhancement candidaterather than an initial deployment priority. While these techniques demonstrated clear performance benefits, they introduce massive architectural complexity compared to more basic vector-based RAG implementation. For the scope of this thesis, the study's key contribution is that manufacturing-specific knowledge representation significantly improves question-answering precision. The research confirms that structured knowledge (which PLM systems inherently possess through product hierarchies, part relationships, and configuration data) can enhance retrieval when properly integrated. The precision-scalability trade-off identified by Wan et al. provides guidance for evaluating when increased architectural complexity justifies performance gains in enterprise PLM contexts.

\subsection*{AI in PLM: Wang et al.'s comprehensive review}

Wang et al. (2021) provided a comprehensive review of AI applications throughout the product lifecycle, examining how intelligent systems can support PLM in the design, manufacturing, and service stages \parencite{wang_artificial_2021}. While their review covers AI technologies broadly rather than RAG systems specifically, it establishes the wider context in which RAG-based documentation assistants operate within PLM environments.

The review identifies several AI applications relevant to understanding where RAG systems fit in the PLM landscape. In the design stage, AI supports market analysis through data mining, rapid conceptual design using case libraries, and design parameter recommendations through expert systems integrated with CAD platforms. The manufacturing stage uses AI for supplier selection, production planning and scheduling, and quality inspection using deep learning and machine vision. The service stage includes intelligent customer service, product status monitoring, and failure prediction using various machine learning approaches.

The service stage applications are particularly relevant for PLM documentation support. Wang et al. describe how intelligent customer service systems combine semantic retrieval using natural language processing to build knowledge bases with expert systems that match knowledge to user queries. This combination closely resembles modern RAG architectures, where retrieval systems identify relevant documentation and generation systems provide contextual answers.

Several challenges identified by Wang et al. directly affect RAG deployment in PLM environments. They emphasize that data quality, algorithm interpretability, and security remain critical concerns for AI in manufacturing contexts. The need to integrate data across different PLM stages and systems presents a significant challenge, as does the requirement to make AI systems understandable and trustworthy for industrial users who may be skeptical of AI recommendations in engineering workflows.

For Sovelia Core, this review highlights important considerations beyond basic document retrieval. A RAG assistant should eventually integrate with other PLM systems, support queries across different lifecycle stages, and provide transparent explanations to build user trust. The emphasis on security and data sovereignty in industrial contexts reinforces the importance of on-premise deployment for PLM environments. While Wang et al.'s study from 2021 does not cover recent RAG developments, the fundamental challenges they identify around data integration, transparency, and trust remain highly relevant for current PLM AI implementations.

\section{Synthesis and implications for Sovelia Core PLM}

Collectively, the case studies detailed in section \ref{sec:case-studies-on-similar-integrations} demonstrate both the technical feasibility and practical challenges of integrating RAG-based AI assistants into complex technical documentation environments. For the scope of this thesis, which focuses on developing a foundational text-based RAG system, the case studies reveal several directly applicable success factors:

\begin{enumerate}
    \item The effectiveness of modern pre-trained embedding models without requiring extensive fine-tuning
    \item The importance of fundamental retrieval parameters (chunk size, overlap, top-k selection) in balancing context chunk completeness with precision
    \item The value of structured document representation (knowledge graphs, hierarchical organization) for further improving retrieval precision
    \item Iterative refinement based on expert user feedback and evaluation metrics
\end{enumerate}

The case studies also highlight more advanced techniques that demonstrate significant performance improvements but introduce substantial implementation complexity. Advanced retrieval strategies such as graph-based traversal (\textcite{knollmeyer_document_2025}) and hybrid KG-vector architectures (\textcite{wang_artificial_2021}) show promise for more complex reasoning tasks and precision-critical applications, while multimodal integration and parameter-efficient fine-tuning techniques from other manufacturing AI research require specialized infrastructure and manual annotation overhead. While these approaches show potential benefits, their requirements for knowledge graph construction, domain-specific ontology definition, manual annotation, specialized training infrastructure, and domain-specific model adaptation position them as future enhancement opportunities rather than initial implementation priorities. The persistent challenges identified across implementations—retrieval precision issues, difficulty handling complex multi-condition queries, the precision-scalability trade-off, and the need for domain-specific evaluation datasets—further reinforce the rationale for establishing a simpler, more maintainable text-based RAG foundation before considering advanced extensions.

For Sovelia Core PLM's initial deployment, these insights suggest prioritizing:

\begin{enumerate}
    \item A straightforward retrieval architecture using pre-trained embedding models
    \item Careful tuning of fundamental parameters (chunking strategy, retrieval depth, keyword density)
    \item Leveraging existing PLM document structure (hierarchies, relationships) where possible
    \item Evaluation using both retrieval and generation metrics
    \item Human-in-the-loop feedback mechanisms for continuous improvement
\end{enumerate}

The on-premise deployment constraint necessitates particular attention to resource-efficient architectures and local data governance. Advanced retrieval strategies such as graph-based traversal, hybrid KG-vector architectures, multimodal capabilities, and parameter-efficient fine-tuning remain valuable directions for future iterations once the baseline text-based system demonstrates practical value and achieves production stability.
