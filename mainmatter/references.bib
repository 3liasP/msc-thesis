
@misc{asai_self-rag_2023,
  title      = {Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  url        = {http://arxiv.org/abs/2310.11511},
  doi        = {10.48550/arXiv.2310.11511},
  shorttitle = {Self-{RAG}},
  abstract   = {Despite their remarkable capabilities, large language models ({LLMs}) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation ({RAG}), an ad hoc approach that augments {LMs} with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes {LM} versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-{RAG}) that enhances an {LM}'s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary {LM} that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the {LM} controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-{RAG} (7B and 13B parameters) significantly outperforms state-of-the-art {LLMs} and retrieval-augmented models on a diverse set of tasks. Specifically, Self-{RAG} outperforms {ChatGPT} and retrieval-augmented Llama2-chat on Open-domain {QA}, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
  number     = {{arXiv}:2310.11511},
  publisher  = {{arXiv}},
  author     = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  urldate    = {2025-10-16},
  date       = {2023-10-17},
  eprinttype = {arxiv},
  eprint     = {2310.11511 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {Preprint PDF:/home/elias/Zotero/storage/PH6DA7T9/Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf;Snapshot:/home/elias/Zotero/storage/7CMMNSRK/2310.html:text/html}
}

@article{lin_generative_2025,
  title        = {Generative {AI} for Intelligent Manufacturing Virtual Assistants in the Semiconductor Industry},
  volume       = {10},
  issn         = {2377-3766},
  url          = {https://ieeexplore.ieee.org/document/10897746},
  doi          = {10.1109/LRA.2025.3544506},
  abstract     = {As semiconductor manufacturing complexity escalates, the intricacy of corresponding manufacturing systems intensifies. These extensive systems necessitate diverse engineering expertise for effective operation and analysis. For instance, yield engineers analyze yield systems, process engineers interpret {FDC} parameters, and equipment engineers monitor device equipment health. Traditional manufacturing systems, reliant on manual data analysis and fixed algorithms, suffer from slow decision-making and limited adaptability. They are susceptible to human error, reactive maintenance, and restricted user interaction confined to technical interfaces and business hours. Additionally, scalability and integration pose significant challenges, inflating operational costs and hampering resource efficiency. This letter introduces an Intelligent Manufacturing Virtual Assistant ({IMVA}) specifically designed for the semiconductor industry. By harnessing the power of Large Language Models ({LLMs}) and {AI} Agents, {IMVA} enhances yield analysis and seamlessly integrates with existing systems and tools. It exhibits high accuracy in defect detection through advanced data analysis and report generation. Furthermore, {IMVA} facilitates natural language interaction, rendering it user-friendly and accessible to non-technical personnel. Consequently, {IMVA} markedly improve operational efficiency and cost-effectiveness compared to traditional manufacturing systems. The efficacy of {IMVA} is demonstrated through the Wide-bandgap ({WBG}) process, showcasing its capability to simplify root cause analysis and provide comprehensive yield reports.},
  pages        = {4132--4139},
  number       = {4},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  author       = {Lin, Chin-Yi and Tsai, Tsung-Han and Tseng, Tzu-Liang},
  urldate      = {2025-10-16},
  date         = {2025-04},
  keywords     = {{AI} agent, Artificial intelligence, Chatbots, Databases, Decision making, Large-language models ({LLM}), Manufacturing, Process control, Production facilities, retrieval-augmented generation ({RAG}), Schedules, Semiconductor device manufacture, Virtual assistants},
  file         = {Full Text PDF:/home/elias/Zotero/storage/9MJH4YY7/Lin et al. - 2025 - Generative AI for Intelligent Manufacturing Virtual Assistants in the Semiconductor Industry.pdf:application/pdf}
}

@inproceedings{shejuti_extended_2025,
  title      = {Extended Abstract: Enhancing Accessibility to {MODTRAN} Documentation: A Chatbot Framework Using Retrieval-Augmented Generation ({RAG})},
  url        = {https://ieeexplore.ieee.org/document/10971575},
  doi        = {10.1109/SoutheastCon56624.2025.10971575},
  shorttitle = {Extended Abstract},
  abstract   = {This work presents the development of a chatbot using Retrieval-Augmented Generation ({RAG}) to streamline access to {MODerate} resolution atmospheric {TRANsmission} ({MODTRAN})'s extensive documentation. By combining document retrieval with a Large Language Model ({LLM}), the chatbot delivers accurate, context-aware answers to user queries. Key resources, including the {MODTRAN}6 User Manual, Algorithm Theoretic Basis Document ({ATBD}), and the frequently asked questions section from the {MODTRAN} website, were processed into a searchable vector database. The evaluation showed that the chatbot effectively provides satisfactory and accurate responses, though occasional extraneous information highlights the need for refinement.},
  eventtitle = {{SoutheastCon} 2025},
  pages      = {1314--1315},
  booktitle  = {{SoutheastCon} 2025},
  author     = {Shejuti, Zarin T. and Deb, Debzani and Dunkel, Emily R.},
  urldate    = {2025-10-16},
  date       = {2025-03},
  note       = {{ISSN}: 1558-058X},
  keywords   = {Chatbots, Databases, Accuracy, Atmospheric modeling, Chatbot, Documentation, Langchain, Large language models, Manuals, {MODTRAN}, {OpenAI}, Refining, Retrieval augmented generation, Vectors},
  file       = {Full Text PDF:/home/elias/Zotero/storage/VVCJHWGA/Shejuti et al. - 2025 - Extended Abstract Enhancing Accessibility to MODTRAN Documentation A Chatbot Framework Using Retri.pdf:application/pdf}
}

@article{nam_lora-tuned_2025,
  title        = {{LoRA}-Tuned Multimodal {RAG} System for Technical Manual {QA}: A Case Study on Hyundai Staria},
  volume       = {15},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/15/15/8387},
  doi          = {10.3390/app15158387},
  shorttitle   = {{LoRA}-Tuned Multimodal {RAG} System for Technical Manual {QA}},
  abstract     = {This study develops a domain-adaptive multimodal {RAG} (Retrieval-Augmented Generation) system to improve the accuracy and efficiency of technical question answering based on large-scale structured manuals. Using Hyundai Staria maintenance documents as a case study, we extracted text and images from {PDF} manuals and constructed {QA}, {RAG}, and Multi-Turn datasets to reflect realistic troubleshooting scenarios. To overcome limitations of baseline {RAG} models, we proposed an enhanced architecture that incorporates sentence-level similarity annotations and parameter-efficient fine-tuning via {LoRA} (Low-Rank Adaptation) using the {bLLossom}-8B language model and {BAAI}-bge-m3 embedding model. Experimental results show that the proposed system achieved improvements of 3.0\%p in {BERTScore}, 3.0\%p in cosine similarity, and 18.0\%p in {ROUGE}-L compared to existing {RAG} systems, with notable gains in image-guided response accuracy. A qualitative evaluation by 20 domain experts yielded an average satisfaction score of 4.4 out of 5. This study presents a practical and extensible {AI} framework for multimodal document understanding, with broad applicability across automotive, industrial, and defense-related technical documentation.},
  pages        = {8387},
  number       = {15},
  journaltitle = {Applied Sciences},
  author       = {Nam, Yerin and Choi, Hansun and Choi, Jonggeun and Kwon, Hyukjin},
  urldate      = {2025-10-16},
  date         = {2025-01},
  langid       = {english},
  note         = {Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {{AI} for structured manuals, domain adaptation, {LoRA}-based fine-tuning, multimodal {RAG}, question-answering system, technical documentation},
  file         = {Full Text PDF:/home/elias/Zotero/storage/J7UEGFFG/Nam et al. - 2025 - LoRA-Tuned Multimodal RAG System for Technical Manual QA A Case Study on Hyundai Staria.pdf:application/pdf}
}

@book{auffarth_generative_2025,
  location    = {Birmingham},
  edition     = {2},
  title       = {Generative {AI} with {LangChain}: Build production-ready {LLM} applications and advanced agents using Python, {LangChain}, and {LangGraph}},
  isbn        = {978-1-83702-201-4},
  url         = {https://learning.oreilly.com/library/view/generative-ai-with/9781837022014/},
  shorttitle  = {Generative {AI} with {LangChain}},
  abstract    = {Dive deep into real-world {LangChain} applications with 'Generative {AI} with {LangChain}'. This comprehensive guide covers everything from multi-agent architectures, testing practices, to...},
  publisher   = {Packt Publishing Limited},
  author      = {Auffarth, Ben},
  editora     = {Kuligin, Leonid},
  editoratype = {collaborator},
  urldate     = {2025-10-16},
  date        = {2025},
  langid      = {english},
  file        = {Snapshot:/home/elias/Zotero/storage/TJU8ZMSE/9781837022014.html:text/html}
}

@article{wang_artificial_2021,
  title        = {Artificial intelligence in product lifecycle management},
  volume       = {114},
  rights       = {© The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2021.},
  issn         = {02683768},
  url          = {https://www.proquest.com/docview/2515489159/abstract/8C4C19515F65471FPQ/1},
  doi          = {10.1007/s00170-021-06882-1},
  abstract     = {Recently, artificial intelligence ({AI}) technology receives extensive attention in the manufacturing field. As the core technology, it generates considerable interest among smart manufacturing and Industry 4.0 strategy. Product lifecycle management ({PLM}) copes with various kinds of engineering, business, and management activities concerning a product throughout its whole lifecycle—from the inception of an intangible concept through the recycling of a finished product. In the context of smart manufacturing, this paper reviews various theories, algorithms, and technologies of {AI} to different stages of {PLM} (i.e., product design, manufacturing, and service). A structured roadmap is presented to navigate the future research and application of {AI} in {PLM}. This paper also discusses the opportunities and challenges of applying {AI} for {PLM}.},
  pages        = {771--796},
  number       = {3},
  journaltitle = {The International Journal of Advanced Manufacturing Technology},
  author       = {Wang, Lei and Zhengchao, Liu and Liu, Ang and Fei, Tao},
  urldate      = {2025-10-17},
  date         = {2021-05},
  note         = {Num Pages: 771-796
                  Place: Heidelberg, Netherlands
                  Publisher: Springer Nature B.V.},
  keywords     = {Artificial intelligence, Manufacturing, Algorithms, Big data, Fourth Industrial Revolution, Life cycle, Life cycle engineering, Lifecycle management, Product design, Product life cycle, Product lifecycle, Product lifecycle management, Production management, Smart manufacturing},
  file         = {Full Text PDF:/home/elias/Zotero/storage/SGHPJ78S/Wang et al. - 2021 - Artificial intelligence in product lifecycle management.pdf:application/pdf}
}

@incollection{stark_product_2015,
  location  = {Cham},
  title     = {Product Lifecycle Management},
  isbn      = {978-3-319-17440-2},
  url       = {https://doi.org/10.1007/978-3-319-17440-2_1},
  abstract  = {Product Lifecycle Management ({PLM}) is the business activity of managing, in the most effective way, a company’s products all the way across their lifecycles; from the very first idea for a product all the way through until it is retired and disposed of. The objective of the first chapter of this book is to provide an introduction to {PLM}, answering the questions: “What is {PLM}?”; “Why {PLM}?”; “When did {PLM} appear”; and “Where is {PLM} used?” This will help those working with {PLM} in a company, including those involved in a company’s {PLM} Initiative, to understand the basics of {PLM} and why it’s so important. It will allow them to participate more fully in the {PLM} Initiative and {PLM} activities. This chapter also aims to give students a basic understanding of {PLM} and its importance in industry. The first part of the chapter gives definitions of {PLM}, a {PLM} Initiative, and the {PLM} Paradigm. The second part of the chapter looks at the meaning of the letters P, L and M in the {PLM} acronym. The third part addresses the scope of {PLM}. It introduces the {PLM} Grid, describes activities within the scope of {PLM}; and identifies the resources managed in {PLM}. The fourth part of the chapter describes the {PLM} Paradigm, detailing concepts, consequences and corollaries. The fifth part looks at the potential benefits, strategic and operational, of {PLM} and a {PLM} Initiative. The sixth part shows how {PLM} has spread since its emergence in 2001. As of 2015, it’s used throughout manufacturing industry and throughout the world. The seventh and final part of the chapter looks at the problems that {PLM} solves and the opportunities it enables.},
  pages     = {1--29},
  booktitle = {Product Lifecycle Management (Volume 1): 21st Century Paradigm for Product Realisation},
  publisher = {Springer International Publishing},
  author    = {Stark, John},
  editor    = {Stark, John},
  urldate   = {2025-10-20},
  date      = {2015},
  langid    = {english},
  doi       = {10.1007/978-3-319-17440-2_1},
  keywords  = {Product Data, Product Family, Product Lifecycle, Product Lifecycle Management, Product Portfolio},
  file      = {Full Text PDF:/home/elias/Zotero/storage/HHZ93B82/Stark - 2015 - Product Lifecycle Management.pdf:application/pdf}
}

@book{stark_product_2015-1,
  edition    = {Third edition.},
  title      = {Product lifecycle management.: (The devil is in the details)},
  volume     = {2},
  isbn       = {978-3-319-24434-1},
  series     = {Decision engineering},
  shorttitle = {Product lifecycle management.},
  abstract   = {This second volume moves beyond a general introduction to product lifecycle management ({PLM}) and its principal elements to provide a more in-depth analysis of the subjects introduced in Volume 1 (21st Century Paradigm for Product Realisation). Providing insights into the emergence of {PLM} and the opportunities it offers, key concepts such as the {PLM} Grid and the {PLM} Paradigm are introduced along with the main components of {PLM} and the associated characteristics, issues and approaches. Detailing the 10 components of {PLM}: objectives and metrics; management and organisation; business processes; people; product data; {PDM} systems; other {PLM} applications; facilities and equipment; methods; and products, it provides examples and best practices. The book concludes with instructions to help readers implement and use {PLM} successfully, including outlining the phases of a {PLM} Initiative: development of {PLM} vision and strategy; documentation of the current situation; description of future scenarios; development of implementation strategies and plans; implementation and use. The main activities, tasks, methods, timing and tools of the different phases are also described.},
  publisher  = {Springer},
  author     = {Stark, John},
  date       = {2015},
  doi        = {10.1007/978-3-319-24436-5},
  keywords   = {Product life cycle}
}

@book{saaksvuori_product_2008,
  location  = {Berlin},
  edition   = {3. ed.},
  title     = {Product lifecycle management},
  isbn      = {978-3-540-78173-8},
  publisher = {Springer},
  author    = {Sääksvuori, Antti and Immonen, Anselmi},
  date      = {2008},
  keywords  = {Product life cycle, Production engineering, Production management}
}

@misc{lewis_retrieval-augmented_2021,
  title      = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  url        = {http://arxiv.org/abs/2005.11401},
  doi        = {10.48550/arXiv.2005.11401},
  abstract   = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  number     = {{arXiv}:2005.11401},
  publisher  = {{arXiv}},
  author     = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  urldate    = {2025-10-20},
  date       = {2021-04-12},
  eprinttype = {arxiv},
  eprint     = {2005.11401 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {Preprint PDF:/home/elias/Zotero/storage/J3C7X9PP/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;Snapshot:/home/elias/Zotero/storage/IHK2NKFD/2005.html:text/html}
}
